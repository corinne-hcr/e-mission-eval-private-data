{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import emission.core.get_database as edb\n",
    "import emission.analysis.modelling.tour_model.similarity as similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import get_request_percentage as grp\n",
    "import get_scores as gs\n",
    "import label_processing as lp\n",
    "import get_users as gu\n",
    "import data_preprocessing as preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import get_plot as plot\n",
    "import emission.core.common as ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_uuid_obj = list(edb.get_profile_db().find({\"install_group\": \"participant\"}, {\"user_id\": 1, \"_id\": 0}))\n",
    "all_users = [u[\"user_id\"] for u in participant_uuid_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore refactor function problem\n",
    "# get valid user list\n",
    "user_ls, valid_users = gu.get_user_ls(all_users, radius)\n",
    "\n",
    "# req_propor_median = []\n",
    "# homogeneity_score = []\n",
    "\n",
    "# collect request percentage for the first or second round (requested trips / total trips) for one user\n",
    "single_first_round_req_pct = []\n",
    "single_second_round_req_pct = []\n",
    "\n",
    "# collect request percentage for the first or second round (requested trips / total trips) for all users\n",
    "all_percentage_first_train = []\n",
    "all_percentage_first_test = []\n",
    "all_percentage_second_train = []\n",
    "all_percentage_second_test = []\n",
    "all_median_pct_first = []\n",
    "\n",
    "# collect homogeneity score for the first/second round for all users\n",
    "all_homogeneity_score_first_train = []\n",
    "all_homogeneity_score_first_test = []\n",
    "all_homogeneity_score_second_train = []\n",
    "all_homogeneity_score_second_test = []\n",
    "all_median_homo_first = []\n",
    "\n",
    "for a in range(1):\n",
    "    user = all_users[a]\n",
    "    filter_trips, trips = preprocess.filter_data(user, radius)\n",
    "    print('user', a + 1, 'filter_trips len', len(filter_trips))\n",
    "\n",
    "    # filter out users that don't have enough valid labeled trips\n",
    "    if not gu.valid_user(filter_trips, trips):\n",
    "        continue\n",
    "    train_idx, test_idx = preprocess.split_data(filter_trips)\n",
    "    print('test_idx ',test_idx)\n",
    "    # choose training/test set to run the model\n",
    "    # this step will use KFold (5 splits) to split the data into different subsets\n",
    "    # - train: training set\n",
    "    # - test: test set\n",
    "    # Here we user a bigger part of the data for testing and a smaller part for tuning\n",
    "    train_data = preprocess.get_subdata(filter_trips, test_idx)\n",
    "    test_data = preprocess.get_subdata(filter_trips, train_idx)\n",
    "\n",
    "    # collect request percentage for a user for the first round\n",
    "    pct_collect_first_train = []\n",
    "    # collect homogeneity score for a user for the first round\n",
    "    homo_collect_first_train = []\n",
    "    # collect request percentage for a user for the second round\n",
    "    pct_collect_second_train = []\n",
    "    # collect homogeneity score for a user for the second round\n",
    "    homo_collect_second_train = []\n",
    "\n",
    "    # run training set first\n",
    "    # collect tuning parameters\n",
    "    coll_low = []\n",
    "    coll_dist_pct = []\n",
    "    colle_tune_score = []\n",
    "\n",
    "    # run every subset\n",
    "    for j in range(len(train_data)):\n",
    "        print('tuning set',j)\n",
    "        sim = similarity.similarity(train_data[j], radius)\n",
    "        filter_trips = sim.data\n",
    "        sim.bin_data()\n",
    "        sim.delete_bins()\n",
    "        bins = sim.bins\n",
    "        bin_trips = sim.newdata\n",
    "        print('bins from the first round ',bins)\n",
    "\n",
    "        # compare the trip orders in bin_trips with those in bins above cutoff\n",
    "        gs.compare_trip_orders(bins, bin_trips, filter_trips)\n",
    "\n",
    "        # create a list idx_labels_track to store indices and labels\n",
    "        # the indices of the items will be the same in the new label list after the second round clustering\n",
    "        # item[0] is the original index of the trip in filter_trips\n",
    "        # item[1] will be the new label after the second round clustering\n",
    "        idx_labels_track = []\n",
    "        for bin in bins:\n",
    "            for ori_idx in bin:\n",
    "                idx_labels_track.append([ori_idx])\n",
    "\n",
    "        # get first round labels\n",
    "        first_labels = []\n",
    "        for b in range(len(bins)):\n",
    "            for trip in bins[b]:\n",
    "                first_labels.append(b)\n",
    "        new_labels = first_labels.copy()\n",
    "        #         print('first round labels train ', new_labels)\n",
    "        first_label_set = list(set(first_labels))\n",
    "\n",
    "        # store first round labels in idx_labels_track list\n",
    "        for i in range(len(first_labels)):\n",
    "            idx_labels_track[i].append(first_labels[i])\n",
    "        # make a copy of idx_labels_track\n",
    "        track = idx_labels_track.copy()\n",
    "        print('track from the first round',track)\n",
    "\n",
    "        # get request percentage for the subset for the first round\n",
    "        percentge_first = float('%.3f' % grp.get_req_pct(new_labels,track,filter_trips,sim))\n",
    "        pct_collect_first_train.append(percentge_first)\n",
    "\n",
    "        # get homogeneity score for the subset for the first round\n",
    "        homo_first = float('%.3f' % gs.score(bin_trips, first_labels))\n",
    "        homo_collect_first_train.append(homo_first)\n",
    "\n",
    "        # tune parameters\n",
    "        highest_score = 0\n",
    "        sel_low = 0\n",
    "        sel_dist_pct = 0\n",
    "        sel_homo_second = 0\n",
    "        sel_percentge_second = 0\n",
    "\n",
    "        for dist_pct in np.arange(0.15, 0.6, 0.02):\n",
    "            for low in range(250, 600):\n",
    "                print('dist_pct is ', dist_pct, 'low is ', low)\n",
    "\n",
    "                # get second round labels\n",
    "                for l in first_label_set:\n",
    "                    print('first label ',l)\n",
    "                    # store second round trips data\n",
    "                    second_round_trips = []\n",
    "                    # create a track to store indices and labels for the second round\n",
    "                    second_round_idx_labels = []\n",
    "                    for index, first_label in enumerate(first_labels):\n",
    "                        if first_label == l:\n",
    "                            second_round_trips.append(bin_trips[index])\n",
    "                            second_round_idx_labels.append([index, first_label])\n",
    "                    # points = []\n",
    "                    point_features = []\n",
    "                    print('second_round_idx_labels',second_round_idx_labels)\n",
    "                    for trip in second_round_trips:\n",
    "                        start = trip.data.start_loc[\"coordinates\"]\n",
    "                        end = trip.data.end_loc[\"coordinates\"]\n",
    "                        distance = trip.data.distance\n",
    "                        duration = trip.data.duration\n",
    "                        # points.append([start[0], start[1], end[0], end[1]])\n",
    "                        point_features.append([start[0], start[1], end[0], end[1], distance, duration])\n",
    "\n",
    "                    x = np.array(point_features)\n",
    "                    \n",
    "                    method = 'single'\n",
    "                    # get labels after two rounds of clustering on common trips\n",
    "                    new_labels = lp.get_new_labels(x, low, dist_pct, second_round_idx_labels, new_labels, method=method)\n",
    "                    print('first round label is ', l,' new_labels ',new_labels)\n",
    "                    track = lp.change_track_labels(track, new_labels)\n",
    "                    print('second round labels ',new_labels)\n",
    "\n",
    "                # get request percentage for the subset for the second round\n",
    "                percentge_second = float('%.3f' % grp.get_req_pct(new_labels, track, filter_trips, sim))\n",
    "\n",
    "                # get homogeneity score for the second round\n",
    "                homo_second = float('%.3f' % gs.score(bin_trips, new_labels))\n",
    "\n",
    "                curr_score = 0.5 * homo_second + 0.5 * (1 - percentge_second)\n",
    "                curr_score = float('%.3f' % curr_score)\n",
    "                if curr_score > highest_score:\n",
    "                    highest_score = curr_score\n",
    "                    sel_low = low\n",
    "                    sel_dist_pct = dist_pct\n",
    "                    sel_homo_second = homo_second\n",
    "                    sel_percentge_second = percentge_second\n",
    "        coll_low.append(sel_low)\n",
    "        coll_dist_pct.append(sel_dist_pct)\n",
    "        colle_tune_score.append(highest_score)\n",
    "        pct_collect_second_train.append(sel_percentge_second)\n",
    "        homo_collect_second_train.append(sel_homo_second)\n",
    "    print('coll_low ', coll_low)\n",
    "    print('coll_dist_pct ', coll_dist_pct)\n",
    "    print('colle_tune_score ', colle_tune_score)\n",
    "\n",
    "\n",
    "    # run test set for evaluation\n",
    "    # collect request percentage for a user for the first round\n",
    "    pct_collect_first_test = []\n",
    "    # collect homogeneity score for a user for the first round\n",
    "    homo_collect_first_test = []\n",
    "    # collect request percentage for a user for the second round\n",
    "    pct_collect_second_test = []\n",
    "    # collect homogeneity score for a user for the second round\n",
    "    homo_collect_second_test = []\n",
    "\n",
    "    # run every subset\n",
    "    for k in range(len(test_data)):\n",
    "        print('test set',k)\n",
    "        sim = similarity.similarity(test_data[k], radius)\n",
    "        filter_trips = sim.data\n",
    "        sim.bin_data()\n",
    "        sim.delete_bins()\n",
    "        bins = sim.bins\n",
    "        bin_trips = sim.newdata\n",
    "        print('bins ', bins)\n",
    "\n",
    "        # compare the trip orders in bin_trips with those in filter_trips above cutoff\n",
    "        gs.compare_trip_orders(bins, bin_trips, filter_trips)\n",
    "\n",
    "        # create a list idx_labels_track to store indices and labels\n",
    "        # the indices of the items will be the same in the new label list after the second round clustering\n",
    "        # item[0] is the original index of the trip in filter_trips\n",
    "        # item[1] will be the new label after the second round clustering\n",
    "        idx_labels_track = []\n",
    "        for bin in bins:\n",
    "            for ori_idx in bin:\n",
    "                idx_labels_track.append([ori_idx])\n",
    "\n",
    "        # get first round labels\n",
    "        first_labels = []\n",
    "        for b in range(len(bins)):\n",
    "            for trip in bins[b]:\n",
    "                first_labels.append(b)\n",
    "        new_labels = first_labels.copy()\n",
    "        first_label_set = list(set(first_labels))\n",
    "\n",
    "        # store first round labels in idx_labels_track list\n",
    "        for i in range(len(first_labels)):\n",
    "            idx_labels_track[i].append(first_labels[i])\n",
    "        # make a copy of idx_labels_track\n",
    "        track = idx_labels_track.copy()\n",
    "\n",
    "        # get request percentage for the subset for the first round\n",
    "        percentge_first = float('%.3f' % grp.get_req_pct(new_labels,track,filter_trips,sim))\n",
    "        pct_collect_first_test.append(percentge_first)\n",
    "\n",
    "        # get homogeneity score for the subset for the first round\n",
    "        homo_first = float('%.3f' % gs.score(bin_trips, new_labels))\n",
    "        homo_collect_first_test.append(homo_first)\n",
    "\n",
    "        low = coll_low[k]\n",
    "        dist_pct = coll_dist_pct[k]\n",
    "\n",
    "        # get second round labels\n",
    "        for l in first_label_set:\n",
    "            # store second round trips data\n",
    "            second_round_trips = []\n",
    "            # create a track to store indices and labels for the second round\n",
    "            second_round_idx_labels = []\n",
    "            for index, first_label in enumerate(first_labels):\n",
    "                if first_label == l:\n",
    "                    second_round_trips.append(bin_trips[index])\n",
    "                    second_round_idx_labels.append([index, first_label])\n",
    "            points = []\n",
    "            point_features = []\n",
    "\n",
    "            for trip in second_round_trips:\n",
    "                start = trip.data.start_loc[\"coordinates\"]\n",
    "                end = trip.data.end_loc[\"coordinates\"]\n",
    "                #                 hour = trip.data.start_local_dt['hour']\n",
    "                distance = trip.data.distance\n",
    "                duration = trip.data.duration\n",
    "                points.append([start[0], start[1], end[0], end[1]])\n",
    "                point_features.append([start[0], start[1], end[0], end[1], distance, duration])\n",
    "\n",
    "            x = np.array(point_features)\n",
    "\n",
    "            method = 'single'\n",
    "            # get labels after two rounds of clustering on common trips\n",
    "            new_labels = lp.get_new_labels(x, low, dist_pct, second_round_idx_labels, new_labels, method=method)\n",
    "            #         print('first round label is ', l,' new_labels ',new_labels)\n",
    "            track = lp.change_track_labels(track, new_labels)\n",
    "        #                 print('second round labels ',new_labels)\n",
    "        print('track',track)\n",
    "        # get request percentage for the subset for the second round\n",
    "        percentge_second = float('%.3f' % grp.get_req_pct(new_labels, track, filter_trips, sim))\n",
    "        pct_collect_second_test.append(percentge_second)\n",
    "\n",
    "        # get homogeneity score for the second round\n",
    "        homo_second = float('%.3f' % gs.score(bin_trips, new_labels))\n",
    "        homo_collect_second_test.append(homo_second)\n",
    "\n",
    "    print('pct_collect_second_test ', pct_collect_second_test)\n",
    "    print('homo_collect_second_test ', homo_collect_second_test)\n",
    "\n",
    "    # collect request percentage for the first round for all users\n",
    "    all_percentage_first_test.append(pct_collect_first_test)\n",
    "\n",
    "    # collect homogeneity score for the first round for all users\n",
    "    all_homogeneity_score_first_test.append(homo_collect_first_test)\n",
    "\n",
    "    # collect request percentage for the second round for all users\n",
    "    all_percentage_second_test.append(pct_collect_second_test)\n",
    "\n",
    "    # collect homogeneity score for the second round for all users\n",
    "    all_homogeneity_score_second_test.append(homo_collect_second_test)\n",
    "\n",
    "#     get_scatter(req_propor_median,homogeneity_score,valid_users)\n",
    "print('all_percentage_first_test', all_percentage_first_test)\n",
    "print('all_homogeneity_score_first_test', all_homogeneity_score_first_test)\n",
    "print('all_percentage_second_test', all_percentage_second_test)\n",
    "print('all_homogeneity_score_second_test', all_homogeneity_score_second_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
