{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.core.get_database as edb\n",
    "import emission.analysis.modelling.tour_model.similarity as similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emission.analysis.modelling.tour_model.get_request_percentage as grp\n",
    "import emission.analysis.modelling.tour_model.get_scores as gs\n",
    "import emission.analysis.modelling.tour_model.label_processing as lp\n",
    "import emission.analysis.modelling.tour_model.get_users as gu\n",
    "import emission.analysis.modelling.tour_model.data_preprocessing as preprocess\n",
    "import evaluation_pipeline as ep\n",
    "import matplotlib.pyplot as plt\n",
    "import get_plot as plot\n",
    "import emission.core.common as ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_uuid_obj = list(edb.get_profile_db().find({\"install_group\": \"participant\"}, {\"user_id\": 1, \"_id\": 0}))\n",
    "all_users = [u[\"user_id\"] for u in participant_uuid_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-novel",
   "metadata": {},
   "source": [
    "### using scipy hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all/valid user list\n",
    "user_ls, valid_users = gu.get_user_ls(all_users, radius)\n",
    "\n",
    "# collect request percentage for the first or second round (requested trips / total trips) for all users\n",
    "all_percentage_first_test = []\n",
    "all_percentage_second_test = []\n",
    "\n",
    "# collect homogeneity score for the first/second round for all users\n",
    "all_homogeneity_score_first_test = []\n",
    "all_homogeneity_score_second_test = []\n",
    "\n",
    "# collect tradeoffs for all users\n",
    "all_tradoffs = []\n",
    "# collect scores for all users\n",
    "all_scores = []\n",
    "\n",
    "for a in range(1):\n",
    "    user = all_users[a]\n",
    "    trips = preprocess.read_data(user)\n",
    "    filter_trips = preprocess.filter_data(trips, radius)\n",
    "    print('user', a + 1, 'filter_trips len', len(filter_trips))\n",
    "\n",
    "    # filter out users that don't have enough valid labeled trips\n",
    "    if not gu.valid_user(filter_trips, trips):\n",
    "        continue\n",
    "    tune_idx, test_idx = preprocess.split_data(filter_trips)\n",
    "\n",
    "    # choose tuning/test set to run the model\n",
    "    # this step will use KFold (5 splits) to split the data into different subsets\n",
    "    # - tune: tuning set\n",
    "    # - test: test set\n",
    "    # Here we user a bigger part of the data for testing and a smaller part for tuning\n",
    "    tune_data = preprocess.get_subdata(filter_trips, test_idx)\n",
    "    test_data = preprocess.get_subdata(filter_trips, tune_idx)\n",
    "    # collect the user labels request percentage from the first round\n",
    "    pct_collect_first = []\n",
    "    # collect the homogeneity score \n",
    "    homo_collect_first = []\n",
    "    pct_collect_second = []\n",
    "    homo_collect_second = []\n",
    "    coll_score = []\n",
    "    coll_tradeoffs = []\n",
    "    \n",
    "    # tune data\n",
    "    for j in range(len(tune_data)):\n",
    "        tuning_parameters = ep.tune(tune_data[j],radius,kmeans=False)\n",
    "        coll_tradeoffs.append(tuning_parameters)\n",
    "    all_tradoffs.append(coll_tradeoffs)\n",
    "\n",
    "    # testing\n",
    "    for k in range(len(test_data)):\n",
    "        tradoffs = coll_tradeoffs[k]\n",
    "        low = tradoffs[0]\n",
    "        dist_pct = tradoffs[1]\n",
    "        homo_first, percentage_first, homo_second, percentage_second, scores = ep.test(test_data[k],radius,low,dist_pct,kmeans=False)\n",
    "        pct_collect_first.append(percentage_first)\n",
    "        homo_collect_first.append(homo_first)\n",
    "        pct_collect_second.append(percentage_second)\n",
    "        homo_collect_second.append(homo_second)\n",
    "        coll_score.append(scores)\n",
    "    all_scores.append(coll_score)\n",
    "    all_percentage_first_test.append(pct_collect_first)\n",
    "    all_percentage_second_test.append(pct_collect_second)\n",
    "    all_homogeneity_score_first_test.append(homo_collect_first)\n",
    "    all_homogeneity_score_second_test.append(homo_collect_second)\n",
    "    \n",
    "print('all_percentage_first_test', all_percentage_first_test)\n",
    "print('all_homogeneity_score_first_test', all_homogeneity_score_first_test)\n",
    "print('all_percentage_second_test', all_percentage_second_test)\n",
    "print('all_homogeneity_score_second_test', all_homogeneity_score_second_test)\n",
    "print('all_scores',all_scores)\n",
    "print('all_tradoffs',all_tradoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-keyboard",
   "metadata": {},
   "source": [
    "### using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all/valid user list\n",
    "user_ls, valid_users = gu.get_user_ls(all_users, radius)\n",
    "\n",
    "# collect request percentage for the first or second round (requested trips / total trips) for all users\n",
    "all_percentage_first_test = []\n",
    "all_percentage_second_test = []\n",
    "\n",
    "# collect homogeneity score for the first/second round for all users\n",
    "all_homogeneity_score_first_test = []\n",
    "all_homogeneity_score_second_test = []\n",
    "\n",
    "# collect tradeoffs for all users\n",
    "all_tradoffs = []\n",
    "# collect scores for all users\n",
    "all_scores = []\n",
    "\n",
    "for a in range(1):\n",
    "    user = all_users[a]\n",
    "    trips = preprocess.read_data(user)\n",
    "    filter_trips = preprocess.filter_data(trips, radius)\n",
    "    print('user', a + 1, 'filter_trips len', len(filter_trips))\n",
    "\n",
    "    # filter out users that don't have enough valid labeled trips\n",
    "    if not gu.valid_user(filter_trips, trips):\n",
    "        continue\n",
    "    tune_idx, test_idx = preprocess.split_data(filter_trips)\n",
    "\n",
    "    # choose tuning/test set to run the model\n",
    "    # this step will use KFold (5 splits) to split the data into different subsets\n",
    "    # - tune: tuning set\n",
    "    # - test: test set\n",
    "    # Here we user a bigger part of the data for testing and a smaller part for tuning\n",
    "    tune_data = preprocess.get_subdata(filter_trips, test_idx)\n",
    "    test_data = preprocess.get_subdata(filter_trips, tune_idx)\n",
    "    # collect the user labels request percentage from the first round\n",
    "    pct_collect_first = []\n",
    "    # collect the homogeneity score \n",
    "    homo_collect_first = []\n",
    "    pct_collect_second = []\n",
    "    homo_collect_second = []\n",
    "    coll_score = []\n",
    "    coll_tradeoffs = []\n",
    "    \n",
    "    # tune data\n",
    "    for j in range(len(tune_data)):\n",
    "        # for tuning, we don't add kmeans for re-clustering. We just need to get tuning parameters\n",
    "        tuning_parameters = ep.tune(tune_data[j],radius,kmeans=False)\n",
    "        coll_tradeoffs.append(tuning_parameters)\n",
    "    all_tradoffs.append(coll_tradeoffs)\n",
    "\n",
    "    # testing\n",
    "    for k in range(len(test_data)):\n",
    "        tradoffs = coll_tradeoffs[k]\n",
    "        low = tradoffs[0]\n",
    "        dist_pct = tradoffs[1]       \n",
    "        # for testing, we add kmeans to re-build the model\n",
    "        homo_first, percentage_first, homo_second, percentage_second, scores = ep.test(test_data[k],radius,low,dist_pct,kmeans=True)\n",
    "        pct_collect_first.append(percentage_first)\n",
    "        homo_collect_first.append(homo_first)\n",
    "        pct_collect_second.append(percentage_second)\n",
    "        homo_collect_second.append(homo_second)\n",
    "        coll_score.append(scores)\n",
    "    all_scores.append(coll_score)\n",
    "    all_percentage_first_test.append(pct_collect_first)\n",
    "    all_percentage_second_test.append(pct_collect_second)\n",
    "    all_homogeneity_score_first_test.append(homo_collect_first)\n",
    "    all_homogeneity_score_second_test.append(homo_collect_second)\n",
    "    \n",
    "print('all_percentage_first_test', all_percentage_first_test)\n",
    "print('all_homogeneity_score_first_test', all_homogeneity_score_first_test)\n",
    "print('all_percentage_second_test', all_percentage_second_test)\n",
    "print('all_homogeneity_score_second_test', all_homogeneity_score_second_test)\n",
    "print('all_scores',all_scores)\n",
    "print('all_tradoffs',all_tradoffs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
