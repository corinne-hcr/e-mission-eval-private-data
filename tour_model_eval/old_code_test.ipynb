{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Our imports\n",
    "import emission.core.get_database as edb\n",
    "import emission.analysis.modelling.tour_model.cluster_pipeline as pipeline\n",
    "import emission.analysis.modelling.tour_model.similarity as similarity\n",
    "import emission.analysis.modelling.tour_model.featurization as featurization\n",
    "import emission.analysis.modelling.tour_model.representatives as representatives\n",
    "import emission.storage.decorations.analysis_timeseries_queries as esda\n",
    "import pandas as pd\n",
    "from numpy import *\n",
    "import confirmed_trips_eval_bins_clusters as evaluation\n",
    "import data_preprocessing as preprocess\n",
    "from sklearn import metrics\n",
    "from pandas.testing import assert_frame_equal\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram,fcluster\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold\n",
    "from past.utils import old_div\n",
    "from tabulate import tabulate\n",
    "import folium\n",
    "import branca.colormap as clm\n",
    "import get_request_percentage as grp\n",
    "import get_scores as gs\n",
    "import label_processing as lp\n",
    "import get_users as gu\n",
    "import data_preprocessing as preprocess\n",
    "import get_plot as plot\n",
    "import emission.core.common as ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(bin_trips,filter_trips,labels_pred,second_round=None):\n",
    "    bin_trips_user_input_df = pd.DataFrame(data=[trip[\"data\"][\"user_input\"] for trip in bin_trips])\n",
    "    bin_trips_user_input_df = evaluation.map_labels(bin_trips_user_input_df, sp2en=None, cvt_pur_mo=True)\n",
    "\n",
    "    # turn all user_input into list without binning\n",
    "    bin_trips_user_input_ls = bin_trips_user_input_df.values.tolist()\n",
    "    # drop duplicate user_input\n",
    "    no_dup_df = bin_trips_user_input_df.drop_duplicates()\n",
    "    # turn non-duplicate user_input into list\n",
    "    no_dup_list = no_dup_df.values.tolist()\n",
    "\n",
    "    # collect labels_true based on user_input\n",
    "    labels_true = []\n",
    "    for trip in bin_trips_user_input_ls:\n",
    "        if trip in no_dup_list:\n",
    "            labels_true.append(no_dup_list.index(trip))\n",
    "\n",
    "    labels_pred = labels_pred\n",
    "    \n",
    "    if second_round:\n",
    "        pass\n",
    "    else:       \n",
    "        # compare the trips order in bins and those in valid_trips using timestamp\n",
    "        bin_trips_ts = pd.DataFrame(data=[trip[\"data\"][\"start_ts\"] for trip in bin_trips])\n",
    "        bin_ls = []\n",
    "        for bin in bins:\n",
    "            for index in bin:\n",
    "                bin_ls.append(index)\n",
    "        bins_ts = pd.DataFrame(data=[filter_trips[i][\"data\"][\"start_ts\"] for i in bin_ls])\n",
    "        # compare two data frames, the program will continue to score calculation if two data frames are the same\n",
    "        assert_frame_equal(bins_ts, bin_trips_ts)\n",
    "    homo_score = metrics.homogeneity_score(labels_true, labels_pred)\n",
    "    return homo_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_labels(x,method,low,dist_pct):\n",
    "    z = linkage(x, method=method, metric='euclidean')\n",
    "    last_d = z[-1][2]\n",
    "    clusters = []\n",
    "    if last_d < low:\n",
    "#         print('last distance ',last_d)\n",
    "        for i in range(len(x)):\n",
    "            clusters.append(0)\n",
    "    else:\n",
    "        max_d = last_d * dist_pct\n",
    "        clusters = fcluster(z, max_d, criterion='distance')\n",
    "#     print('clusters is ',clusters)    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_labels(x,low,dist_pct,second_round_idx_labels,new_labels,method=None):\n",
    "    idx_label = second_round_idx_labels.copy()\n",
    "    second_labels= get_second_labels(x,method,low,dist_pct) \n",
    "    for i in range(len(second_labels)):\n",
    "        index = idx_label[i][0]\n",
    "        new_label = idx_label[i][1]\n",
    "        # concatenate labels from two rounds\n",
    "        new_label = int(str(new_label) + str(second_labels[i]))\n",
    "        for k in range(len(new_labels)):\n",
    "            if k == index:\n",
    "                new_labels[k] = new_label\n",
    "                break\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group similar trips according to new_labels, store the original indices \n",
    "def group_similar_trips(new_labels,track):\n",
    "    bin_sim_trips = []\n",
    "    for trip_index,label in enumerate(new_labels):\n",
    "        added = False\n",
    "        for bin in bin_sim_trips:\n",
    "            if label == new_labels[bin[0]]:\n",
    "                bin.append(trip_index)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            bin_sim_trips.append([trip_index])\n",
    "    # using track to replace the current indices with original indicies\n",
    "    for bin in bin_sim_trips:\n",
    "        for i in range(len(bin)):\n",
    "            bin[i] = track[bin[i]][0]\n",
    "    return bin_sim_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_track_labels(track,new_labels):\n",
    "    for i in range(len(new_labels)):\n",
    "        track[i][1] = new_labels[i]\n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect requested trips and common trips(no need to request) indices above cutoff\n",
    "def requested_trips_ab_cutoff(new_bins,filter_trips):   \n",
    "    ab_trip_ls = []\n",
    "    no_req_trip_ls = []\n",
    "    for bin in new_bins:\n",
    "        early_trip_index,index = evaluation.find_first_trip(filter_trips,bin)\n",
    "        ab_trip_ls.append(early_trip_index)\n",
    "        \n",
    "        for k in range(len(bin)):\n",
    "            if k != index:\n",
    "                no_req_trip_idx = bin[k]\n",
    "                no_req_trip_ls.append(no_req_trip_idx)\n",
    "    return ab_trip_ls,no_req_trip_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect requested trips indices below cutoff\n",
    "def requested_trips_bl_cutoff(sim):\n",
    "    # bins below cutoff\n",
    "    bl_bins = sim.below_cutoff\n",
    "    \n",
    "    # collect requested trips indices below cutoff\n",
    "    bl_trip_ls = []\n",
    "    for bin in bl_bins:\n",
    "        for trip_index in bin:\n",
    "            bl_trip_ls.append(trip_index)\n",
    "    return  bl_trip_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all requested trips indices\n",
    "def get_requested_trips(new_bins,filter_trips,sim):\n",
    "    ab_trip_ls,no_req_trip_ls = requested_trips_ab_cutoff(new_bins,filter_trips)\n",
    "    bl_trip_ls = requested_trips_bl_cutoff(sim)\n",
    "    req_trips_ls=ab_trip_ls+bl_trip_ls\n",
    "    return req_trips_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get request percentage based on the number of requested trips and the total number of trips\n",
    "def get_req_pct(new_labels,track,filter_trips):\n",
    "    # - new_bins: bins with original indices of similar trips\n",
    "    new_bins = group_similar_trips(new_labels,track)\n",
    "    req_trips = get_requested_trips(new_bins,filter_trips,sim)\n",
    "    pct = len(req_trips)/len(filter_trips)\n",
    "    return pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KFold (n_splits=5) to split the data into 5 models (5 training sets, 5 test sets)\n",
    "def split_data(filter_trips):\n",
    "    X = []\n",
    "    for trip in filter_trips:\n",
    "        start = trip.data.start_loc[\"coordinates\"]\n",
    "        end = trip.data.end_loc[\"coordinates\"]\n",
    "        distance = trip.data.distance\n",
    "        duration = trip.data.duration   \n",
    "        X.append([start[0], start[1], end[0], end[1],distance,duration])\n",
    "        \n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state=9)\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_idx.append(train_index)\n",
    "        test_idx.append(test_index)\n",
    "    return train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a set of data(training/test set) after splitting \n",
    "# - splited_indices: pass in train set indices or test set indices\n",
    "def get_subdata(sim,train_test_set):\n",
    "    collect_sub_data = []\n",
    "    for train_test_subset in train_test_set:\n",
    "        sub_data = []\n",
    "        for idx in train_test_subset:\n",
    "            sub_data.append(sim.data[idx])\n",
    "        collect_sub_data.append(sub_data)\n",
    "    return collect_sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter(percentage,homo_score,valid_users):\n",
    "    x=percentage\n",
    "    y=homo_score\n",
    "    v=valid_users\n",
    "    cmp = cm.get_cmap('Dark2', len(valid_users))\n",
    "\n",
    "    sc = []\n",
    "    for i in range(len(valid_users)):\n",
    "        for n in range(len(x[i])):           \n",
    "            point = plt.scatter(x[i][n], y[i][n], color=cmp.colors[i], s=70, alpha=0.7)\n",
    "        sc.append(point)\n",
    "    plt.legend(sc,v,markerscale=0.8,scatterpoints=1,bbox_to_anchor=(1.23,1))\n",
    "    plt.xlabel('user input request percentage',fontsize=16)\n",
    "    plt.ylabel('homogeneity score',fontsize=16)\n",
    "    plt.xticks(np.arange(0.4,1.1,step=0.1),fontsize=14)\n",
    "    plt.yticks(np.arange(0.2,1.1,step=0.1),fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_cluster_map(cluster,filter_trips,bins):\n",
    "    color_map = clm.linear.Set1_07.to_step(len(bins), index=[i for i in range(len(bins)+1)])\n",
    "    first_trip = filter_trips[cluster[0]]\n",
    "    map = folium.Map(location=[first_trip.data.start_loc[\"coordinates\"][1], first_trip.data.start_loc[\"coordinates\"][0]],\n",
    "                   zoom_start=12, max_zoom=30, control_scale=True)\n",
    "\n",
    "    zoom_points = []\n",
    "    for curr_trip_index in cluster:\n",
    "        for i in range(len(bins)):          \n",
    "            curr_trip = filter_trips[curr_trip_index]\n",
    "            if curr_trip_index in bins[i]:\n",
    "                # We need polyline to plot the trip according to start_loc and end_loc\n",
    "                # Flip indices because points are in geojson (i.e. lon, lat),folium takes [lat,lon]\n",
    "#                 layer = folium.PolyLine(\n",
    "#                     [[curr_trip.data.start_loc[\"coordinates\"][1], curr_trip.data.start_loc[\"coordinates\"][0]],\n",
    "#                      [curr_trip.data.end_loc[\"coordinates\"][1], curr_trip.data.end_loc[\"coordinates\"][0]]], weight=2,\n",
    "#                     color=color_map(i+1))\n",
    "#                 layer.add_to(map)\n",
    "                start_points = folium.CircleMarker([curr_trip.data.start_loc[\"coordinates\"][1], curr_trip.data.start_loc[\"coordinates\"][0]],\n",
    "                                    radius=3,color='green',fill=True,fill_color='green',fill_opacity=1)\n",
    "#                 start_points.add_to(map)\n",
    "#\n",
    "                end_points = folium.CircleMarker([curr_trip.data.end_loc[\"coordinates\"][1], curr_trip.data.end_loc[\"coordinates\"][0]],\n",
    "                                    radius=3,color='red',fill=True,fill_color='red',fill_opacity=1)\n",
    "                end_points.add_to(map)\n",
    "                zoom_points.append([curr_trip.data.start_loc[\"coordinates\"][1],\n",
    "                                    curr_trip.data.start_loc[\"coordinates\"][0]])\n",
    "                zoom_points.append([curr_trip.data.end_loc[\"coordinates\"][1],\n",
    "                                    curr_trip.data.end_loc[\"coordinates\"][0]])\n",
    "    df = pd.DataFrame(zoom_points, columns=['Lat', 'Long'])\n",
    "    sw = df[['Lat', 'Long']].min().values.tolist()\n",
    "    ne = df[['Lat', 'Long']].max().values.tolist()\n",
    "    map.fit_bounds([sw, ne])\n",
    "\n",
    "#     map.add_child(color_map)\n",
    "    return map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_uuid_obj = list(edb.get_profile_db().find({\"install_group\": \"participant\"}, {\"user_id\": 1, \"_id\": 0}))\n",
    "all_users = [u[\"user_id\"] for u in participant_uuid_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for refactoring problem\n",
    "# get valid user list\n",
    "user_ls,valid_users = gu.get_user_ls(all_users, radius)\n",
    "\n",
    "# req_propor_median = []\n",
    "# homogeneity_score = []\n",
    "\n",
    "# collect request percentage for the first/second round (requested trips/total trips) for one user\n",
    "single_first_round_req_pct = []\n",
    "single_second_round_req_pct = []\n",
    "\n",
    "\n",
    "# collect request percentage for the first/second round (requested trips/total trips) for all users\n",
    "all_percentage_first_train = []\n",
    "all_percentage_first_test = []\n",
    "all_percentage_second_train = []\n",
    "all_percentage_second_test= []\n",
    "all_median_pct_first = []\n",
    "\n",
    "# collect homogeneity score for the first/second round for all users\n",
    "all_homogeneity_score_first_train = []\n",
    "all_homogeneity_score_first_test = []\n",
    "all_homogeneity_score_second_train = []\n",
    "all_homogeneity_score_second_test = []\n",
    "all_median_homo_first = []\n",
    "\n",
    "for a in range(1):\n",
    "    user = all_users[a]\n",
    "    filter_trips, trips = preprocess.filter_data(user, radius)\n",
    "    print('user',a+1,'filter_trips len', len(filter_trips))\n",
    "\n",
    "    # filter out users that don't have enough valid labeled trips\n",
    "    if not gu.valid_user(filter_trips, trips):\n",
    "        continue\n",
    "    train_idx, test_idx = split_data(filter_trips)  \n",
    "    print('test_idx ',test_idx)\n",
    "    # choose training/test set to run the model\n",
    "    # this step will use KFold (3 splits) to split the data into different subsets\n",
    "    # - train: training set\n",
    "    # - test: test set    \n",
    "    # Here we user a bigger part of the data for testing and a smaller part for tuning\n",
    "#     train_data = get_subdata(sim,test_idx)\n",
    "#     test_data = get_subdata(sim,train_idx)\n",
    "    train_data = preprocess.get_subdata(filter_trips, test_idx)\n",
    "    test_data = preprocess.get_subdata(filter_trips, train_idx)\n",
    "\n",
    "\n",
    "    \n",
    "    # collect request percentage for a user for the first round\n",
    "    pct_collect_first_train = []\n",
    "    # collect homogeneity score for a user for the first round\n",
    "    homo_collect_first_train = []\n",
    "    # collect request percentage for a user for the second round\n",
    "    pct_collect_second_train = []\n",
    "    # collect homogeneity score for a user for the second round\n",
    "    homo_collect_second_train = []\n",
    "    \n",
    "    # run training set first\n",
    "    # collect tuning parameters\n",
    "    coll_low = []\n",
    "    coll_dist_pct = []\n",
    "    colle_tune_score = []\n",
    "\n",
    "    # run every subset\n",
    "    for j in range(len(train_data)):\n",
    "        print('tuning set',j)\n",
    "        sim = similarity.similarity(train_data[j], radius)\n",
    "        filter_trips = sim.data\n",
    "        sim.bin_data()\n",
    "        sim.delete_bins()\n",
    "        bins = sim.bins\n",
    "        bin_trips = sim.newdata\n",
    "        print('bins from the first round ',bins)\n",
    "        \n",
    "        # compare the trip orders in bin_trips with those in bins above cutoff\n",
    "        gs.compare_trip_orders(bins, bin_trips, filter_trips)\n",
    "\n",
    "        # create a list idx_labels_track to store indices and labels\n",
    "        # the indices of the items will be the same in the new label list after the second round clustering\n",
    "        # item[0] is the original index of the trip in filter_trips\n",
    "        # item[1] will be the new label after the second round clustering\n",
    "        idx_labels_track = []\n",
    "        for bin in bins:\n",
    "            for ori_idx in bin:\n",
    "                idx_labels_track.append([ori_idx])\n",
    "\n",
    "        # get first round labels\n",
    "        first_labels = []\n",
    "        for b in range(len(bins)):\n",
    "            for trip in bins[b]:\n",
    "                first_labels.append(b)\n",
    "        new_labels = first_labels.copy()\n",
    "#         print('first round labels train ', new_labels)\n",
    "        first_label_set = list(set(first_labels))\n",
    "\n",
    "        # store first round labels in idx_labels_track list\n",
    "        for i in range(len(first_labels)):\n",
    "            idx_labels_track[i].append(first_labels[i])\n",
    "        # make a copy of idx_labels_track\n",
    "        track = idx_labels_track.copy()\n",
    "        print('track from the first round',track)\n",
    "        \n",
    "        # get request percentage for the subset for the first round\n",
    "        percentge_first = float('%.3f' % get_req_pct(new_labels,track,filter_trips))\n",
    "        pct_collect_first_train.append(percentge_first)\n",
    "        \n",
    "        # get homogeneity score for the subset for the first round\n",
    "        homo_first = float('%.3f' % score(bin_trips,filter_trips,new_labels))\n",
    "        homo_collect_first_train.append(homo_first)\n",
    "        \n",
    "        # tune parameters\n",
    "        highest_score = 0\n",
    "        sel_low = 0\n",
    "        sel_dist_pct = 0\n",
    "        sel_homo_second = 0\n",
    "        sel_percentge_second = 0\n",
    "\n",
    "\n",
    "        for dist_pct in np.arange(0.15, 0.6, 0.02):\n",
    "            for low in range(250,600):\n",
    "                print('dist_pct is ', dist_pct, 'low is ', low)\n",
    "        \n",
    "                # get second round labels\n",
    "                for l in first_label_set:\n",
    "                    print('first label ',l)\n",
    "                    # store second round trips data\n",
    "                    second_round_trips = []\n",
    "                    # create a track to store indices and labels for the second round\n",
    "                    second_round_idx_labels = []          \n",
    "                    for index, first_label in enumerate(first_labels):\n",
    "                        if first_label == l:\n",
    "                            second_round_trips.append(bin_trips[index])\n",
    "                            second_round_idx_labels.append([index,first_label])\n",
    "                    points = []\n",
    "                    point_features = []\n",
    "                    print('second_round_idx_labels',second_round_idx_labels)\n",
    "\n",
    "                    for trip in second_round_trips:\n",
    "                        start = trip.data.start_loc[\"coordinates\"]\n",
    "                        end = trip.data.end_loc[\"coordinates\"]\n",
    "        #                 hour = trip.data.start_local_dt['hour']\n",
    "                        distance = trip.data.distance\n",
    "                        duration = trip.data.duration   \n",
    "                        points.append([start[0], start[1], end[0], end[1]])\n",
    "                        point_features.append([start[0], start[1], end[0], end[1],distance,duration])\n",
    "\n",
    "                    x = np.array(point_features)\n",
    "\n",
    "                    method = 'single'\n",
    "                    # get labels after two rounds of clustering on common trips\n",
    "                    new_labels = get_new_labels(x,low,dist_pct,second_round_idx_labels,new_labels,method=method)\n",
    "                    print('first round label is ', l,' new_labels ',new_labels)\n",
    "                    track = change_track_labels(track,new_labels)\n",
    "                    print('second round labels ',new_labels)\n",
    "\n",
    "                # get request percentage for the subset for the second round\n",
    "                percentge_second = float('%.3f' % get_req_pct(new_labels,track,filter_trips))            \n",
    "\n",
    "                # get homogeneity score for the second round\n",
    "                homo_second = float('%.3f' % score(bin_trips,filter_trips,new_labels,second_round=True))\n",
    "                   \n",
    "                curr_score = 0.5 * homo_second + 0.5 * (1 - percentge_second)\n",
    "                curr_score = float('%.3f' % curr_score)\n",
    "                if curr_score > highest_score:\n",
    "                    highest_score = curr_score\n",
    "                    sel_low = low\n",
    "                    sel_dist_pct = dist_pct\n",
    "                    sel_homo_second = homo_second\n",
    "                    sel_percentge_second = percentge_second\n",
    "        coll_low.append(sel_low)\n",
    "        coll_dist_pct.append(sel_dist_pct)\n",
    "        colle_tune_score.append(highest_score)\n",
    "        pct_collect_second_train.append(sel_percentge_second)\n",
    "        homo_collect_second_train.append(sel_homo_second)\n",
    "    print('coll_low ',coll_low)\n",
    "    print('coll_dist_pct ',coll_dist_pct)\n",
    "    print('colle_tune_score ',colle_tune_score)\n",
    "        \n",
    "\n",
    "\n",
    "               \n",
    "#         # get user input request proportion in a day\n",
    "#         propor_single_user = get_request_proportion(new_bins,filter_trips,sim)\n",
    "\n",
    "#         # get user input request proportion median in a day\n",
    "#         median = np.median(propor_single_user)\n",
    "        \n",
    "#         # collect medians for every user\n",
    "#         req_propor_median.append(median)\n",
    "       \n",
    "   \n",
    "    \n",
    "    # run test set for evaluation\n",
    "    # collect request percentage for a user for the first round\n",
    "    pct_collect_first_test = []\n",
    "    # collect homogeneity score for a user for the first round\n",
    "    homo_collect_first_test= []\n",
    "    # collect request percentage for a user for the second round\n",
    "    pct_collect_second_test = []\n",
    "    # collect homogeneity score for a user for the second round\n",
    "    homo_collect_second_test = []\n",
    "                   \n",
    "    # run every subset\n",
    "    for k in range(len(test_data)):\n",
    "        print('test set',k)\n",
    "        sim = similarity.similarity(test_data[k], radius)\n",
    "        filter_trips = sim.data\n",
    "        sim.bin_data()\n",
    "        sim.delete_bins()\n",
    "        bins = sim.bins\n",
    "        bin_trips = sim.newdata\n",
    "        print('bins ', bins)\n",
    "\n",
    "        \n",
    "        # compare the trip orders in bin_trips with those in filter_trips above cutoff\n",
    "        gs.compare_trip_orders(bins, bin_trips, filter_trips)\n",
    "\n",
    "        # create a list idx_labels_track to store indices and labels\n",
    "        # the indices of the items will be the same in the new label list after the second round clustering\n",
    "        # item[0] is the original index of the trip in filter_trips\n",
    "        # item[1] will be the new label after the second round clustering\n",
    "        idx_labels_track = []\n",
    "        for bin in bins:\n",
    "            for ori_idx in bin:\n",
    "                idx_labels_track.append([ori_idx])\n",
    "\n",
    "        # get first round labels\n",
    "        first_labels = []\n",
    "        for b in range(len(bins)):\n",
    "            for trip in bins[b]:\n",
    "                first_labels.append(b)\n",
    "        new_labels = first_labels.copy()\n",
    "#         print('first round labels ', new_labels)\n",
    "        first_label_set = list(set(first_labels))\n",
    "\n",
    "        # store first round labels in idx_labels_track list\n",
    "        for i in range(len(first_labels)):\n",
    "            idx_labels_track[i].append(first_labels[i])\n",
    "        # make a copy of idx_labels_track\n",
    "        track = idx_labels_track.copy()\n",
    "        \n",
    "        # get request percentage for the subset for the first round\n",
    "        percentge_first = float('%.3f' % get_req_pct(new_labels,track,filter_trips))\n",
    "        pct_collect_first_test.append(percentge_first)\n",
    "        \n",
    "        # get homogeneity score for the subset for the first round\n",
    "        homo_first = float('%.3f' % score(bin_trips,filter_trips,new_labels))\n",
    "        homo_collect_first_test.append(homo_first)\n",
    "                   \n",
    "        low = coll_low[k]\n",
    "        dist = coll_dist_pct[k]\n",
    "                   \n",
    "        # get second round labels\n",
    "        for l in first_label_set:\n",
    "            # store second round trips data\n",
    "            second_round_trips = []\n",
    "            # create a track to store indices and labels for the second round\n",
    "            second_round_idx_labels = []          \n",
    "            for index, first_label in enumerate(first_labels):\n",
    "                if first_label == l:\n",
    "                    second_round_trips.append(bin_trips[index])\n",
    "                    second_round_idx_labels.append([index,first_label])\n",
    "            points = []\n",
    "            point_features = []\n",
    "\n",
    "            for trip in second_round_trips:\n",
    "                start = trip.data.start_loc[\"coordinates\"]\n",
    "                end = trip.data.end_loc[\"coordinates\"]\n",
    "    #                 hour = trip.data.start_local_dt['hour']\n",
    "                distance = trip.data.distance\n",
    "                duration = trip.data.duration   \n",
    "                points.append([start[0], start[1], end[0], end[1]])\n",
    "                point_features.append([start[0], start[1], end[0], end[1],distance,duration])\n",
    "\n",
    "            x = np.array(point_features)\n",
    "\n",
    "            method = 'single'\n",
    "            # get labels after two rounds of clustering on common trips\n",
    "            new_labels = get_new_labels(x,low,dist_pct,second_round_idx_labels,new_labels,method=method)\n",
    "    #         print('first round label is ', l,' new_labels ',new_labels)\n",
    "            track = change_track_labels(track,new_labels)\n",
    "    #                 print('second round labels ',new_labels)\n",
    "\n",
    "        # get request percentage for the subset for the second round\n",
    "        print('track',track)\n",
    "        percentge_second = float('%.3f' % get_req_pct(new_labels,track,filter_trips))            \n",
    "        pct_collect_second_test.append(percentge_second)\n",
    "                   \n",
    "        # get homogeneity score for the second round\n",
    "        homo_second = float('%.3f' % score(bin_trips,filter_trips,new_labels,second_round=True))\n",
    "        homo_collect_second_test.append(homo_second)\n",
    "        \n",
    "    print('pct_collect_second_test ',pct_collect_second_test)\n",
    "    print('homo_collect_second_test ',homo_collect_second_test)\n",
    "\n",
    "                   \n",
    "    # collect request percentage for the first round for all users\n",
    "    all_percentage_first_test.append(pct_collect_first_test)\n",
    "    \n",
    "    # collect homogeneity score for the first round for all users\n",
    "    all_homogeneity_score_first_test.append(homo_collect_first_test)   \n",
    "    \n",
    "    # collect request percentage for the second round for all users\n",
    "    all_percentage_second_test.append(pct_collect_second_test)\n",
    "        \n",
    "    # collect homogeneity score for the second round for all users\n",
    "    all_homogeneity_score_second_test.append(homo_collect_second_test)\n",
    "    \n",
    "    \n",
    "print('all_percentage_first_test', all_percentage_first_test)\n",
    "print('all_homogeneity_score_first_test', all_homogeneity_score_first_test)\n",
    "print('all_percentage_second_test', all_percentage_second_test)\n",
    "print('all_homogeneity_score_second_test', all_homogeneity_score_second_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
